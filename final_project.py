# -*- coding: utf-8 -*-
"""FINAL_PROJECT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZgeIs2TRydJZLgt3NECT_inBTpdvKnbE

# Projeto final de Machine Learning
***
## Professor : Adriano Veloso
## Aluna :        Ana Carolina Gontijo Graça
## Matrícula :  2016074242

# Tema do trabalho : Classificação de imagens de raio-X torácico para identificação de pneumonia.
### Será analisado um banco de dados de imagens de raio-X toráxico que são classificadas como imagens de pulmões com e sem pneumonia. O treinamento será feito com redes __CNN__ e a escolha de parâmetros vai ser feita de acordo com os testes instânciados neste notebook.

### São utilizados 2 tipos de __CNN__ s que serão descritos melhor em breve.
"""

# Instalação da função de redimensionamento das imagens
!pip install python-resize-image

aux_path = ''
# Suporte para o uso de documentos do drive no notebook
#from google.colab import drive
#drive.mount('/content/drive', force_remount=True)
#aux_path = '/content/drive/My Drive/'

# Imports de bibliotecas para analise e tratamento dos dados
import numpy as np
from PIL import Image
import pandas as pd
import matplotlib.pyplot as plt
from resizeimage import resizeimage
import os
from keras.utils import to_categorical
import matplotlib as mpl

# Imports para processamento de imagem no keras
from keras.preprocessing.image import load_img
from keras.preprocessing.image import save_img
from keras.preprocessing.image import img_to_array

# Imports para desenvolvimento das redes neurais
from keras import layers
from keras import optimizers
from keras import models

# Imports de utilitários do sklearn para analise da acurácia
from sklearn import metrics

"""## Primeira parte ( NÃO SE APLICA ): Tratando as imagens
### De forma a facilitar o upload e o uso das imagens durante o aprendizado primeiro iremos reduzir todas as imagens para que seu lado de maior comprimento tenha no máximo 500 pixels.

##### (Isso foi realizado previamente ao envio no banco de dados original, baixado do kaggle)
"""

#for dir_aux in []:
#    for filename in os.listdir('chest-xray-pneumonia/chest_xray/chest_xray/'+dir_aux):
#        if filename[-4:] == 'jpeg':
#            print(dir_aux+filename)
#            with open('chest-xray-pneumonia/chest_xray/chest_xray/'+dir_aux+filename, 'r+b') as f:
#                with Image.open(f) as image:
#                    try:
#                        if image.size[0] > image.size[1]:
#                            cover = resizeimage.resize_width(image, 500)
#                        else:
#                            cover = resizeimage.resize_height(image, 500)
#                        plt.imshow(cover)
#                        cover.save('resized_xrays/'+dir_aux+filename, image.format)
#                    except:
#                        continue

"""# Segunda parte: Preparando as imagens para o keras"""

# Alguns exemplos dos dados para verificação
NORMAL = []
PNEUMONIA = []
i = 0
for filename in os.listdir(aux_path+'resized_xrays/test/NORMAL/'):
    with open(aux_path+'resized_xrays/test/NORMAL/'+filename, 'r+b') as f:
        with Image.open(f) as image:
            img = resizeimage.resize_contain(
                image,[100,100],bg_color=(0, 0, 0, 0)).convert('L')
        NORMAL.append(img)
    i+=1
    if i == 10:
        break
            
            
            
i = 0
for filename in os.listdir(aux_path+'resized_xrays/test/PNEUMONIA/'):
    with open(aux_path+'resized_xrays/test/PNEUMONIA/'+filename, 'r+b') as f:
        with Image.open(f) as image:
            img = resizeimage.resize_contain(
                image,[100,100],bg_color=(0, 0, 0, 0)).convert('L')
        PNEUMONIA.append(img)
    i+=1
    if i == 10:
        break
          
n_cols = 5
fig_1, axes_1 = plt.subplots(1, n_cols, figsize=(20, 5))
#fig_1.suptitle("Imagens sem pneumonia")
for j in range(n_cols):
    img_i = j
    axes_1[j].imshow(NORMAL[img_i], cmap="gray")
    axes_1[j].axis("off")
    
fig_2, axes_2 = plt.subplots(1, n_cols, figsize=(20, 5))

#fig_2.suptitle("Imagens com pneumonia")

for j in range(n_cols):
    img_i = j    
    axes_2[j].imshow(PNEUMONIA[img_i], cmap="gray")
    axes_2[j].axis("off")

# Carregamento e pre-processamento das imagens, separando entre validação e 
# treino. As imagens são redimensionadas para serem contidas em quadrados de 
# 100 x 100 pixels, com fundo preto.
data = {'train':[], 'train_class':[], 'val':[], 'val_class':[]}
classes = {'NORMAL':0, 'PNEUMONIA':1}

for dir_aux in ['train','val']:
    for classify in ['NORMAL','PNEUMONIA']:
        for filename in os.listdir(aux_path+'resized_xrays/'+dir_aux+'/'+
                                   classify+'/'):
            with open(aux_path+'resized_xrays/'+dir_aux+'/'+classify+'/'+
                      filename, 'r+b') as f:
                if filename[-4:] == 'jpeg':
                    with Image.open(f) as image:
                        print(filename)
                        img = resizeimage.resize_contain(
                            image,[100,100],bg_color=(0, 0, 0, 0)).convert('L')
                        data[dir_aux].append(img_to_array(img))
                        data[dir_aux+'_class'].append(np.array(classes[classify]))

                
for dir_aux in ['test']:
    for classify in ['NORMAL','PNEUMONIA']:
        for filename in os.listdir(aux_path+'resized_xrays/'+dir_aux+'/'+
                                   classify+'/'):
            with open(aux_path+'resized_xrays/'+dir_aux+'/'+classify+'/'+
                      filename, 'r+b') as f:
                if filename[-4:] == 'jpeg':
                    with Image.open(f) as image:
                        print(filename)
                        img = resizeimage.resize_contain(
                            image,[100,100],bg_color=(0, 0, 0, 0)).convert('L')
                        data['val'].append(img_to_array(img))
                        data['val_class'].append(np.array(classes[classify]))
            
# Transforma todas as listas em np arrays para uso nas redes neurais
data['train'] = np.array(data['train'])
data['train_class'] = np.array(data['train_class'])
data['val'] = np.array(data['val'])
data['val_class'] = np.array(data['val_class'])

"""# Terceira parte: Treinamentos dos CNNs"""

# Função para plotar os resultados do treino de acurácia e perda
def plot_results_by_history(history):
    # Gera graficos do resultado do treino
    mpl.rc("font", **{"size": 14})
    fig, axes = plt.subplots(1, 2, figsize=(17, 7))

    # Loss
    axes[0].plot(range(1, len(history.history["loss"])+1), 
                 history.history["loss"], label="Train Loss", color="red", lw=3)
    axes[0].plot(range(1, len(history.history["val_loss"])+1), 
                 history.history["val_loss"], label="Train Loss", 
                 color="black", lw=3)
    axes[1].set_ylim(bottom=0)
    axes[0].set_ylabel("Loss")
    axes[0].set_xlabel("Epoch")
    axes[0].set_xticks(range(1, len(history.history["acc"])+1))
    axes[0].legend()

    # Accuracy
    axes[1].plot(range(1, len(history.history["acc"])+1), 
                 history.history["acc"], label="Train ACC", color="red", lw=3)
    axes[1].plot(range(1, len(history.history["val_acc"])+1), 
                 history.history["val_acc"], label="Teste ACC", 
                 color="black", lw=3)
    axes[1].set_ylim(0.5,1)
    axes[1].set_ylabel("Accuracy")
    axes[1].set_xlabel("Epoch")
    axes[1].set_xticks(range(1, len(history.history["acc"])+1))
    axes[1].legend()
    plt.show()

# Normaliza a intensidade dos pixels para o intervalo [0, 1]
data['train'] = data['train']/255
data['val'] = data['val']/255

# Primeiro tipo de CNN:
# conv -> pool -> conv -> pool -> conv -> pool -> dense -> dense -> sigmoid
def run_cnn_1(conv1, conv2, conv3, train, test, img_shape, lr=0.001, fc1=5, 
              do=0.5, n_classes=None, model=None):
    
    model_input = layers.Input(shape=img_shape, name="Input")
    

    x = layers.Conv2D(2**(conv1), kernel_size=(3, 3), activation="relu", 
                      padding="same", name="Conv1")(model_input)
    x = layers.MaxPooling2D(pool_size=(2, 2), name="Pool1")(x)
    x = layers.SpatialDropout2D(rate=do, name='Dropout2D_1')(x)
    x = layers.Conv2D(2**(conv2), kernel_size=(3, 3), activation="relu", 
                      padding="same", name="Conv2")(x)
    x = layers.MaxPooling2D(pool_size=(2, 2), name="Pool2")(x)
    x = layers.SpatialDropout2D(rate=do, name='Dropout2D_2')(x)
    x = layers.Conv2D(2**(conv3), kernel_size=(3, 3), activation="relu", 
                      padding="same", name="Conv4")(x)
    x = layers.MaxPooling2D(pool_size=(2, 2), name="Pool3")(x)
    x = layers.SpatialDropout2D(rate=do, name='Dropout2D_3')(x)

    x = layers.Flatten(name="Flatten")(x)

    x = layers.Dense(2**(fc1), activation="relu", name="FC1")(x)
    
    x = layers.Dense(2**(fc1-1), activation="relu", name="FC2")(x)
    
    x = layers.Dropout(rate=do,name='Dropout')(x)
    
    model_output = layers.Dense(n_classes, activation='sigmoid', 
                                name="Output")(x)

    model = models.Model(model_input, model_output)

    # Inicializa o otimizador. Adam é uma variação do SGD
    optimizer = optimizers.Adam(lr=lr)

    # Compila o modelo, escolhendo a funcao de perda e a metrica principal
    model.compile(optimizer, loss="binary_crossentropy", metrics=["accuracy"])

    model.summary()

    # Treina por 10 epocas com mini-batches de 32 exemplos.
    # A API aceita tambem dados de validacao, que sao usadas ao final 
    # de cada epoca para medir a metrica principal
    

    history = model.fit(train[0], train[1], batch_size=8, 
                        epochs=20, validation_data=test)
    
    return history, model

for i in [5,6,7]:
    for j in [7,8]:
        for k in [8,9]:
            history, model = run_cnn_1(conv1=i, conv2=j, conv3=k,
                                       train=(data['train'],
                                              data['train_class']),
                                       test=(data['val'],
                                             data['val_class']), 
                                       img_shape=(100,100, 1), n_classes=1)
            plot_results_by_history(history)

# Segundo tipo de CNN:
# conv -> conv -> pool ->  conv -> conv -> pool -> dense -> dense -> sigmoid
def run_cnn_2(conv1, conv2, conv3, conv4, train, test, img_shape, fc1=5, do=0.5, 
              lr=0.001, n_classes=None, model=None):
    
    model_input = layers.Input(shape=img_shape, name="Input")

    x = layers.Conv2D(2**(conv1), kernel_size=(3, 3), activation="relu", 
                      padding="same", name="Conv1")(model_input)
    x = layers.Conv2D(2**(conv2), kernel_size=(3, 3), activation="relu", 
                      padding="same", name="Conv2")(x)
    x = layers.MaxPooling2D(pool_size=(2, 2), name="Pool1")(x)
    x = layers.SpatialDropout2D(rate=do, name='Dropout2D_1')(x)
    x = layers.Conv2D(2**(conv3), kernel_size=(3, 3), activation="relu", 
                      padding="same", name="Conv3")(x)
    x = layers.Conv2D(2**(conv4), kernel_size=(3, 3), activation="relu", 
                      padding="same", name="Conv4")(x)
    x = layers.MaxPooling2D(pool_size=(2, 2), name="Pool2")(x)
    x = layers.SpatialDropout2D(rate=do, name='Dropout2D_2')(x)

    x = layers.Flatten(name="Flatten")(x)

    x = layers.Dense(2**(fc1), activation="relu", name="FC1")(x)
    
    x = layers.Dense(2**(fc1-1), activation="relu", name="FC2")(x)
    
    model_output = layers.Dense(n_classes, activation='sigmoid', 
                                name="Output")(x)

    model = models.Model(model_input, model_output)

    # Inicializa o otimizador. Adam é uma variação do SGD
    optimizer = optimizers.Adam(lr=lr)

    # Compila o modelo, escolhendo a funcao de perda e a metrica principal
    model.compile(optimizer, loss="binary_crossentropy", metrics=["accuracy"])

    model.summary()

    # Treina por 10 epocas com mini-batches de 32 exemplos.
    # A API aceita tambem dados de validacao, que sao usadas ao final 
    # de cada epoca para medir a metrica principal
    

    history = model.fit(train[0], train[1], batch_size=16, 
                        epochs=20, validation_data=test)
    
    return history, model

for i in [5,6,7]:
    for j in [7,8]:
        for k in [8,9]:
            history, model = run_cnn_2(conv1=i, conv2=j, conv3=j, conv4=k,
                                       train=(data['train'],
                                              data['train_class']), 
                                       test=(data['val'],
                                             data['val_class']), 
                                       img_shape=(100,100, 1), n_classes=1)
            plot_results_by_history(history)

